# Deep-learning-decodes-species-specific-codon-usage-signatures-in-Brassica-from-coding-sequences
# CDS Filtering Pipeline for Brassica Species
# 01_CDS_Data_Validation.py
This repository contains a Python script (01_CDS_Data_Validation.py) to filter coding sequences (CDS) from four Brassica species (B. juncea, B. napus, B. oleracea, B. rapa) downloaded from EnsemblPlants (release-61). The script validates FASTA-formatted CDS against strict biological criteria:

Length divisible by 3

Standard DNA bases only (A/T/C/G)

Valid start (ATG) and stop codons (TAA, TAG, TGA)

No internal stop codons

Correct translation to amino acids

Data Sources Raw CDS files were retrieved from:

B. juncea: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_juncea/cds/

B. napus: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_napus/cds/

B. oleracea: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_oleracea/cds/

B. rapa: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_rapa/cds/

Run the script:
# python 01_CDS_Data_Validation.py Input: FASTA files in input_dir (configured in script).

Output: Filtered FASTA files + statistics CSV (filtering_stats.csv).

Dependencies Python 3.x

Biopython (pip install biopython)

Output Filtered sequences: Valid CDS per species (.fasta).

Statistics: Detailed filtering metrics (pass/fail counts per criterion). Codon Frequency Analysis

# 02_CDS_Codon_Freq_CSV.py
# The script 02_CDS_Codon_Freq_CSV.py computes codon usage frequencies from filtered CDS sequences (generated by CDS_Data_Validation.py) and outputs a combined CSV file for downstream analysis.
Workflow Input: Filtered FASTA files (from Final_Filtered directory).

Processing:

Counts occurrences of all 64 codons per sequence.

Labels species numerically (sorted by sequence count, descending).

Output: A CSV file (combined_codon_frequencies_labeled.csv) with columns:

Species: Original species name (e.g., Brassica_napus.AST_PRJEB5043_v1.cds.all_filtered).

Label: Numeric ID (1 = highest sequence count).

Sequence_ID: FASTA record identifier.

Columns for all 64 codons (e.g., AAA, AAC, ..., TTT) with raw counts.

Example Output (First 3 Rows) Species Label Sequence_ID AAA AAC ... TTT Brassica_napus.AST_PRJEB5043_v1.cds... 1 CDY69013 6 6 ... 4 Brassica_napus.AST_PRJEB5043_v1.cds... 1 CDY71688 10 8 ... 6 Brassica_napus.AST_PRJEB5043_v1.cds... 1 CDY71689 6 1 ... 3 (Full CSV includes all 64 codons and all sequences.)

Usage
python 02_CDS_Codon_Freq_CSV.py Input Directory: Configure input_dir in the script (default: Final_Filtered).

Output: CSV saved to input_dir/combined_codon_frequencies_labeled.csv.

Dependencies Python 3.x

Biopython (pip install biopython)

Pandas (pip install pandas)

Notes Reproducibility: The script automatically sorts species by sequence count (descending) and assigns labels.

Integration: Use this CSV as input for statistical analyses or visualizations (e.g., PCA, t-SNE and UMAP).
# 03_PCA_CUB_plot.py
This script performs Principal Component Analysis (PCA) on codon frequency data. It creates a scatter plot where each point represents a gene sequence, colored by species. The plot displays the first two principal components with their explained variance percentages, helping identify major patterns in codon usage variation across species.

# 04_t_SNE.py
This script applies t-Distributed Stochastic Neighbor Embedding (t-SNE) for non-linear dimensionality reduction. It uses Barnes-Hut approximation for efficient computation and creates hexbin density plots to visualize gene distribution patterns. The output is saved as both PDF and PNG files with publication-ready formatting.

# 05_Umap_plot.py
This script implements Uniform Manifold Approximation and Projection (UMAP) for visualizing high-dimensional codon usage data. With optimized parameters for biological data, it generates hexbin density plots that preserve both local and global data structure, saved as vector graphics suitable for publications.
# Installation
bash
pip install pandas scikit-learn matplotlib seaborn umap-learn
# Data Format
Note: The same combined_codon_frequencies_labeled.csv file data will be used to prouduce PCA, t-SNE and UMAP plts
# Important Note for the following  Neural Network Codes:

Data Preparation Update
Before running any of the neural network classification scripts (06_dropout.py through 12_deepbelief.py), please follow these data preparation steps:

1. Modify Your CSV File:
Original CSV Structure: Should contain Sequence_ID, Species, and Label columns

Required Modification: Exclude Sequence_ID column, Species column rename Label column to Species

Result: File should contain only codon frequency columns with a single Species label column 
# 06_dropout.py (Dropout Neural Network)
# Dropout Neural Network for Species Classification

This script implements a Dropout-regularized Deep Neural Network (DNN) for multi-species classification using codon usage data. It performs 10-fold cross-validation to ensure robust model evaluation.

Key Features:

10-fold cross-validation with stratified sampling

Dropout regularization (0.3 rate) to prevent overfitting

Early stopping with patience of 5 epochs

Comprehensive metrics: accuracy, precision, recall, F1-score, and Matthews Correlation Coefficient

Saves best model and all fold models

Model Architecture:

Input layer → Dense(128, ReLU) → Dropout(0.3)

Dense(64, ReLU) → Dropout(0.3)

Dense(32, ReLU) → Dropout(0.3)

Output layer: Softmax for multi-class classification

Outputs:

Saved models for each fold and best overall model

Average confusion matrix plot

Training/validation accuracy curves

CSV file with average metrics (DO_Results.csv)

NumPy files with training histories and matrices

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow
# 07_DNN_L2_Reg.py (Deep Neural Network with L2 Regulrization)
L2-Regularized Neural Network for Species Classification

This script implements a Deep Neural Network with L2 regularization (weight decay) for multi-species classification using codon usage data. It combines dropout and L2 regularization techniques for improved generalization.

Key Features:

L2 regularization (λ=0.001) applied to all dense layers

Combined with dropout regularization (0.3 rate)

10-fold cross-validation with stratified sampling

Early stopping with patience of 5 epochs

Comprehensive performance metrics tracking

Model Architecture:

Input layer → Dense(128, ReLU, L2=0.001) → Dropout(0.3)

Dense(64, ReLU, L2=0.001) → Dropout(0.3)

Dense(32, ReLU, L2=0.001) → Dropout(0.3)

Output layer: Softmax for multi-class classification

Outputs:

Saved models for each fold and best overall model (Best_L2_DNN_Model.h5)

Average confusion matrix plot

Training/validation accuracy curves

CSV file with average metrics (DL2_NN_Metrics_Avg.csv)

NumPy files with training histories and matrices

Key Differences from Dropout-only Model:

Adds L2 weight decay (0.001) to prevent large weight values

Dual regularization approach for enhanced generalization

Models saved with prefix L2_DropoutNN_foldX.h5

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow
Leaky ReLU Neural Network for Species Classification
# 08_Leaky_Relu.py (Leaky ReLU Neural Network)
This script implements a neural network using Leaky ReLU activation functions for multi-species classification. Leaky ReLU prevents the "dying ReLU" problem by allowing small gradients when units are not active.

Key Features:

Leaky ReLU activation (α=0.01) in all hidden layers

Dropout regularization (0.2 rate) for overfitting prevention

10-fold cross-validation with comprehensive metrics tracking

Separate test data evaluation (10% holdout)

Generates both raw and normalized confusion matrices

Model Architecture:

Input layer → Dense(128) → LeakyReLU(0.01) → Dropout(0.2)

Dense(64) → LeakyReLU(0.01) → Dropout(0.2)

Dense(32) → LeakyReLU(0.01) → Dropout(0.2)

Output layer: Softmax for multi-class classification

Unique Features:

Test Data Evaluation: Separate 10% test set evaluation not used in cross-validation

Two Confusion Matrices: Raw counts and normalized percentages

Complete Metrics: Both cross-validation and test set performance metrics

Class Labels: Uses species names (Wheat, Rice, Barley, BD) in confusion matrices

Outputs:

Saved models for each fold and best model (Best_LeakyReLU_Model.h5)

Cross-validation and test set confusion matrices

Training/validation accuracy curves

CSV files: LeakyReLU_Metrics_Avg.csv (CV) and Test_Metrics.csv

Both raw and normalized confusion matrix plots

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow
# 09_Shallow.py (Shallow Neural Network)
Shallow Neural Network for Species Classification

This script implements a shallow (single hidden layer) neural network for multi-species classification. This minimalist architecture serves as a baseline comparison against deeper networks, demonstrating the effectiveness of simpler models.

Key Features:

Single hidden layer architecture (64 neurons)

ReLU activation in hidden layer

Dropout regularization (0.2 rate) for generalization

10-fold cross-validation with comprehensive evaluation

Large font formatting for presentation-ready plots

Model Architecture:

Input layer → Dense(64, ReLU) → Dropout(0.2)

Output layer: Softmax for multi-class classification

Unique Features:

Simplified Architecture: Only one hidden layer for computational efficiency

Baseline Model: Serves as reference point for comparing deeper architectures

Presentation-Ready Plots: Larger fonts (size 16-20) for better visibility in presentations

Thicker Plot Lines: Linewidth=4 for enhanced visual clarity

Outputs:

Saved models for each fold and best model (Best_Shallow_NN_Model.h5)

Average confusion matrix plot

Training/validation accuracy curves with enhanced visibility

CSV file with average metrics (Shallow_NN_Metrics_Avg.csv)

NumPy files with training histories and matrices

Comparison Points:

Vs. Deep Networks: Evaluates if depth significantly improves performance

Vs. Regularized Networks: Tests if simpler architecture needs less regularization

Computational Efficiency: Faster training due to fewer parameters

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow
# 10_MLP.py (Multilayer Perceptron)
Multi-Layer Perceptron (MLP) with Leaky ReLU and L2 Regularization

This script implements a sophisticated Multi-Layer Perceptron network combining Leaky ReLU activations with strong L2 regularization for robust multi-species classification. This represents the most complex architecture in the neural network comparison series.

Key Features:

Deep architecture with 5 layers (256-128-64-32-output)

Leaky ReLU activation (α=0.1) in all hidden layers

Strong L2 regularization (λ=0.01) on all dense layers

Dropout regularization (0.3 rate) at each layer

10-fold cross-validation for reliable performance estimation

Model Architecture:

Input → Dense(256, L2=0.01) → LeakyReLU(0.1) → Dropout(0.3)

Dense(128, L2=0.01) → LeakyReLU(0.1) → Dropout(0.3)

Dense(64, L2=0.01) → LeakyReLU(0.1) → Dropout(0.3)

Dense(32, L2=0.01) → LeakyReLU(0.1) → Dropout(0.3)

Output layer: Softmax for multi-class classification

Unique Features:

Deepest Architecture: Most layers among all neural network implementations

Double Regularization: Combines L2 weight decay with dropout for maximum generalization

Advanced Activation: Leaky ReLU prevents gradient vanishing in deep networks

Strong Regularization: Higher L2 coefficient (0.01 vs 0.001 in L2-only model)

Outputs:

Saved models for each fold and best model (Best_MLP_Model.h5)

Average confusion matrix plot

Training/validation accuracy curves

CSV file with average metrics (implied - similar to other models)

NumPy files with training histories and matrices

Comparison Points:

Vs. Simple Networks: Tests if depth and complexity improve performance

Vs. Single Regularization: Evaluates combined regularization effectiveness

Computational Load: Heaviest architecture requiring most training time

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow
# 11_RBFNN.py (Radial Basis Function) 
Radial Basis Function Neural Network (RBFNN) for Species Classification

This script implements a Radial Basis Function Neural Network, a type of neural network that uses radial basis functions as activation functions. RBFNNs are particularly effective for pattern recognition and classification tasks with well-separated clusters.

Key Features:

Custom RBF layer with fixed centroids using K-means clustering (50 centroids)

Gaussian RBF activation with gamma parameter (γ=0.1)

Follows same output format as DBN/RBFN for consistency

Separate test data evaluation (10% holdout)

Generates both raw and normalized confusion matrices

Architecture:

Custom RBFLayer: 50 centroids initialized via K-means, Gaussian RBF activation

Output layer: Dense with Softmax for multi-class classification

Unique Features:

Fixed Centroids: Centroids determined by K-means and remain non-trainable

Radial Basis Functions: Uses Gaussian activation based on distance from centroids

Test Data Evaluation: Separate 10% test set evaluation

Two Confusion Matrices: Raw counts and normalized percentages

Class Labels: Uses species names (Wheat, Rice, Barley, BD)

Technical Details:

Centroids initialized using K-means clustering on training data

Gaussian RBF: φ(x) = exp(-γ * ||x - μ||²) where γ=0.1

Fixed centroids (non-trainable) for stability

Outputs match DBN format for easy comparison

Outputs:

Saved models for each fold and best model (Best_RBFN_Model.h5)

Cross-validation and test set confusion matrices

Training/validation accuracy curves

CSV files: RBFN_Metrics_Avg.csv (CV) and Test_Metrics.csv

Both raw and normalized confusion matrix plots

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow, sklearn.cluster
# 12_deepbelief.py
Deep Belief Network (DBN) Simulation for Species Classification

This script implements a Deep Belief Network-inspired architecture using stacked fully connected layers for multi-species classification. While not a true DBN with Restricted Boltzmann Machines, it simulates the deep layered structure characteristic of DBNs.

Key Features:

Deep architecture with 3 hidden layers (128-64-32 neurons)

ReLU activation functions in all hidden layers

Dropout regularization (0.2 rate) for each layer

10-fold cross-validation with comprehensive evaluation

Early stopping with patience of 5 epochs

Model Architecture:

Input layer → Dense(128, ReLU) → Dropout(0.2)

Dense(64, ReLU) → Dropout(0.2)

Dense(32, ReLU) → Dropout(0.2)

Output layer: Softmax for multi-class classification

Unique Features:

DBN-Inspired Structure: Simulates the deep hierarchical feature learning of true DBNs

Balanced Regularization: Moderate dropout (0.2) suitable for deep architectures

Consistent Format: Follows same output structure as other models for comparison

Feature Hierarchy: Captures hierarchical patterns through multiple processing layers

Technical Note:
While called "Deep Belief Network," this implementation uses standard feedforward neural networks rather than the traditional Restricted Boltzmann Machine (RBM) pretraining and fine-tuning of true DBNs. It maintains the deep layered structure that characterizes DBN architectures.

Outputs:

Saved models for each fold and best model (Best_DBN_Model.h5)

Average confusion matrix plot

Training/validation accuracy curves

CSV file with average metrics (DBN_Metrics_Avg.csv)

NumPy files with training histories and matrices

Dependencies:

python
numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow

